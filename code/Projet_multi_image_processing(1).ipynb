{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75412e27-9194-421a-98f3-e25c3f411939",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310cf591-dd0b-4453-86a6-25382a9983ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#Image processing\n",
    "from skimage import measure\n",
    "from skimage import filters\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import segmentation\n",
    "from skimage.segmentation import watershed, clear_border\n",
    "from skimage import measure, color\n",
    "from scipy.optimize import curve_fit\n",
    "from skimage.morphology import extrema\n",
    "import matplotlib.patches as mpatches\n",
    "import skimage as ski\n",
    "import pandas as pd\n",
    "# file management imports|\n",
    "import os  ### only for count of images from dir, can be removed later\n",
    "# model imports for deep learning \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "import keras\n",
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# image processing imports\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,  img_to_array, load_img\n",
    "from sklearn import metrics\n",
    "#for confusion matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import itertools    \n",
    "#for displaying images when predicting class\n",
    "from PIL import Image, ImageOps\n",
    "#for rounding up fitting model for steps_per_epoch\n",
    "import math\n",
    "from keras.models import load_model,Model\n",
    "#Xplique\n",
    "from xplique.attributions import GradientInput,GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f5f64-7395-46e9-90f3-55c64ea3a851",
   "metadata": {},
   "source": [
    "# Preprocess and binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a57fbc4-3e1b-49d1-84f8-1372f3fc5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturated contrast/brightness modification\n",
    "def contrast_brightness_modify(img, alpha, beta):\n",
    "    # Add bias and gain to an image with saturation arithmetics. \n",
    "    # Varialbes :\n",
    "        # name_img : You need to put the name of the image file hear (png, jpg...)\n",
    "        # alpha : You choose the good paramatre for the contrast control = multiple of each pixel's intensity\n",
    "        # beta : You choose the value for the brightness control = offset intensity for each bit   \n",
    "    new_img = img * alpha + beta\n",
    "    new_img[new_img < 0] = 0\n",
    "    new_img[new_img > 255] = 255\n",
    "    return new_img.astype(np.uint8)\n",
    "    \n",
    "# Histogram equalization algorithm\n",
    "# This function clip a percentage of the histogram from the bottom.\n",
    "# User can add a clipping method from the top.\n",
    "def automatic_brightness_and_contrast(image, clip_hist_percent=1,inverse_color=False):\n",
    "    # This fuction can auto-adjust the brightness and contrast of the image using histogram equalization method and return \n",
    "    # the alpha (contrast) and the beta (brightness) coefficients of the image\n",
    "\n",
    "    # Use inverse_color to make the back ground black and object white\n",
    "    \n",
    "    # The most important parameter of this function is clip_hist_percent.\n",
    "    # This parameter can be exploited and added to main function if necessary. \n",
    "\n",
    "    # Convert to gray scale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if inverse_color:\n",
    "        gray = 255 - gray\n",
    "        \n",
    "    # Calculate grayscale histogram\n",
    "    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n",
    "    hist_size = len(hist)\n",
    "    \n",
    "    # Calculate cumulative distribution from the histogram\n",
    "    accumulator = []\n",
    "    accumulator.append(float(hist[0]))\n",
    "    for index in range(1, hist_size):\n",
    "        accumulator.append(accumulator[index -1] + float(hist[index]))\n",
    "    \n",
    "    # Locate points to clip\n",
    "    maximum = accumulator[-1]\n",
    "    clip_hist_percent *= (maximum/100.0)\n",
    "    clip_hist_percent /= 2.0\n",
    "    \n",
    "    # Locate left cut\n",
    "    minimum_gray = 0\n",
    "    while accumulator[minimum_gray] < clip_hist_percent:\n",
    "        minimum_gray += 1\n",
    "    \n",
    "    # Locate right cut\n",
    "    maximum_gray = hist_size -1\n",
    "    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n",
    "        maximum_gray -= 1\n",
    "    \n",
    "    # Calculate alpha and beta values\n",
    "    alpha = 255 / (maximum_gray - minimum_gray)\n",
    "    beta = -minimum_gray * alpha\n",
    "    \n",
    "    # Calculate new histogram with desired range and show histogram \n",
    "    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])\n",
    "    plt.figure()\n",
    "    plt.title('Gray level histogram')\n",
    "    plt.xlabel('Pixel value')\n",
    "    plt.ylabel('Number of pixels')\n",
    "    plt.plot(hist)\n",
    "    plt.plot(new_hist)\n",
    "    plt.xlim([0,256])\n",
    "    plt.show()\n",
    "\n",
    "    auto_result = contrast_brightness_modify(gray, alpha=alpha, beta=beta)\n",
    "    return (auto_result, alpha, beta)\n",
    "\n",
    "# Gamma correction. Not yet utilized in this algorithm.\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "# Image process algorithm.\n",
    "def image_treatment(name_img, inverse_color = False ,kernel_morpho = 5,open_iter=1,close_iter=1,clear_bder = False ):\n",
    "    # This function take an image from the repertory and do the image processing, include:\n",
    "        # Show image\n",
    "        # Denoise the Gaussian noise\n",
    "        # Control the contrast, the brightness and the color for some images\n",
    "        # Show the denoised image\n",
    "        # Do the morphological operation\n",
    "        # Return the treated image\n",
    "    # Varialbes :\n",
    "        # inverse_color : Some image need to be converted their background to black one and we treat the NP in white\n",
    "            # inverse_color = false (default : no need to convert)\n",
    "            # inverse_color = true\n",
    "    # This function returns the processed image and the alpha, beta coefficients. If the binary image doesn't correspond well to what we want \n",
    "    # to process, use the contrast_brightness_modify() function to adjust alpha and beta.\n",
    "    img = cv2.imread(name_img)\n",
    "   # Denoising image\n",
    "    img1 = cv2.fastNlMeansDenoising(img , h=10 , templateWindowSize=7 , searchWindowSize=21)\n",
    "    # Automatic contrast/birghtness adjustment\n",
    "    gray,alpha,beta = automatic_brightness_and_contrast(img1,1,inverse_color)\n",
    "    # Denoising again since the noise is amplified after correcting contrast and brightness\n",
    "    gray = cv2.fastNlMeansDenoising(gray , h=10 , templateWindowSize=7 , searchWindowSize=21)\n",
    "    # Gaussian blur\n",
    "    gray = cv2.GaussianBlur(gray , (7,7),sigmaX=1,sigmaY=1)\n",
    "    # Sharpen image\n",
    "    sharpen_filter=np.array([[-0,-1,-0],\n",
    "                 [-1,5,-1],\n",
    "                [-0,-1,-0]])\n",
    "\n",
    "    sharp_image=cv2.filter2D(gray,-1,sharpen_filter)\n",
    "    # Threshold the image using Binary + Otsu method\n",
    "    _, binary = cv2.threshold(sharp_image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    # Filter kernel for morphological operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_morpho,kernel_morpho))\n",
    "    \n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=open_iter) # Opening morphology\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=close_iter) # CLosing morphology\n",
    "\n",
    "    # Clear object in contact with border\n",
    "    if clear_bder:\n",
    "        binary = clear_border(binary)\n",
    "\n",
    "    # Plotting result\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    gs = fig.add_gridspec(2,2)\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original image\",weight=\"bold\")\n",
    "    \n",
    "    ax2.imshow(sharp_image,cmap='gray')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Processed image\", weight=\"bold\")\n",
    "    \n",
    "    ax3.imshow(binary,cmap='gray')\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title(\"Binarized image\",weight=\"bold\")\n",
    "   \n",
    "    return binary,alpha,beta\n",
    "\n",
    "# Image process algorithm.\n",
    "# Same principle as the previous algorithm except that this function require manual adjustment of contrast and brightness\n",
    "def image_treatment_manuel(name_img, inverse_color = False ,kernel_morpho = 5,open_iter=1,close_iter=1,clear_bder = False,alpha=1,beta=0 ):\n",
    "    # This function take an image from the repertory and do the image processing, include:\n",
    "        # Show image\n",
    "        # Denoise the Gaussian noise\n",
    "        # Control the contrast, the brightness and the color for some images\n",
    "        # Show the denoised image\n",
    "        # Do the morphology\n",
    "        # Return the treated image\n",
    "    # Varialbes :\n",
    "        # inverse_color : Some image need to be converted their background to black one and we treat the NP in white\n",
    "            # inverse_color = false (default : no need to convert)\n",
    "            # inverse_color = true\n",
    "    # This function returns the processed image and the alpha, beta coefficients. If the binary image doesn't correspond well to what we want \n",
    "    # to process, use the contrast_brightness_modify() function to adjust alpha and beta.\n",
    "    # Convert to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if inverse_color : \n",
    "        gray = 255 - gray\n",
    "\n",
    "    gray = cv2.fastNlMeansDenoising(gray , h=10 , templateWindowSize=7 , searchWindowSize=21)\n",
    "    \n",
    "    gray = contrast_brightness_modify(gray,alpha,beta)\n",
    "    \n",
    "\n",
    "    gray = cv2.fastNlMeansDenoising(gray , h=10 , templateWindowSize=7 , searchWindowSize=21)\n",
    "\n",
    "    gaussian_blur = cv2.GaussianBlur(gray , (7,7),sigmaX=1,sigmaY=1)\n",
    "    \n",
    "    sharpen_filter=np.array([[-0,-1,-0],\n",
    "                 [-1,5,-1],\n",
    "                [-0,-1,-0]])\n",
    "\n",
    "    sharp_image=cv2.filter2D(gray,-1,sharpen_filter)\n",
    "\n",
    "    _, binary = cv2.threshold(sharp_image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_morpho,kernel_morpho))\n",
    "    \n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=open_iter)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=close_iter)\n",
    "    \n",
    "    if clear_bder:\n",
    "        binary = clear_border(binary)\n",
    "        \n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    gs = fig.add_gridspec(2,2)\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original image\",weight=\"bold\")\n",
    "    \n",
    "    ax2.imshow(sharp_image,cmap='gray')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Processed image\", weight=\"bold\")\n",
    "    \n",
    "    ax3.imshow(binary,cmap='gray')\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title(\"Binarized image\",weight=\"bold\")\n",
    "   \n",
    "    return binary,alpha,beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c606f55-0a8e-4b7f-9458-82300ac3765c",
   "metadata": {},
   "source": [
    "# Scale bar detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc6926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale bar detection \n",
    "# This fuction finds the object using Canny edge detection \n",
    "def detect_scale_bar(image_path,physical_length,inverse_color = False):\n",
    "    # Variables :\n",
    "        # image_path : the path directory to the image need to detect the scale bar\n",
    "        # physical_length : length in nm\n",
    "    # Image input + grayscale + inverse color if needed\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if inverse_color : \n",
    "        gray_image = 255- gray_image\n",
    "    # Gaussian blur\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "    # Canny edge detection\n",
    "    edges = cv2.Canny(blurred_image, 50, 150)\n",
    "    # Contours detection based on edges detected\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    max_area = 0\n",
    "    scale_bar_contour = None\n",
    "\n",
    "    # Sort out the contour with the largest area \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            scale_bar_contour = contour\n",
    "    # Draw detected scale bar\n",
    "    if scale_bar_contour is not None:\n",
    "        x, y, w, h = cv2.boundingRect(scale_bar_contour)\n",
    "        cv2.rectangle(gray_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cut_image = np.copy(image)\n",
    "        cut_image[y:y+h, x:x+w] = [0, 255, 0]\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(1,2,figsize=(15, 30))\n",
    "        ax[0].imshow(image,cmap='gray')\n",
    "        ax[0].axis('off') \n",
    "        ax[0].set_title(\"Scale bar image\")\n",
    "\n",
    "        handles = mpatches.Patch(color='lime', label='Detected scale bar')\n",
    "        ax[1].legend(handles=[handles],bbox_to_anchor=( 1, -0.2),loc='upper right',\n",
    "                      ncols=2, borderaxespad=0)\n",
    "        ax[1].set_title(\"Detected scale bar\")\n",
    "        ax[1].imshow(cut_image,cmap='gray')\n",
    "        ax[1].axis('off')        \n",
    "    # Return the width of detected scale bar\n",
    "        scale_bar_pixels = w\n",
    "        scale_bar_ratio = physical_length/scale_bar_pixels\n",
    "        return scale_bar_ratio\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d6c13-bf3a-43c5-a8bd-45a2a1b7a23c",
   "metadata": {},
   "source": [
    "# Watershed Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5157845-7780-4b71-83a6-c3fdca54e603",
   "metadata": {},
   "source": [
    "### 1. Watershed with local extremum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f4fd26-7b23-4a30-be6b-7a0622c233b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watershed with local extremum\n",
    "def NP_segmentation_local_max(name_img, min_distance,dist_max_threshold = 0.4,erode_iter=1,open_iter=0,kernel_size = 3) :\n",
    "    # Variables:\n",
    "        # name_img : The treated image from image_treatment function, you can choose anothe treated image with another method\n",
    "        # min_distance : To find the local maximum point => just count the point with distance > min_distance\n",
    "            # min_distance depend on the pixel distance between NPs in the image\n",
    "            # We usually meet the case : min_d = 1..10 or min_d = 30..40\n",
    "    # Filter kernel for morphological operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size,kernel_size))\n",
    "    # Eroding morphology - Reduce isotropically object size\n",
    "    # This operation helps a lot in revealing the line between object\n",
    "    # But it can also clear out object with small size\n",
    "    eroded = cv2.erode(name_img, kernel, iterations=erode_iter) \n",
    "    eroded = cv2.morphologyEx(eroded, cv2.MORPH_OPEN, kernel, iterations=open_iter)\n",
    "    # Distance map transform with edt (Euclidean Distance Transform)\n",
    "    distance = ndi.distance_transform_edt(eroded)\n",
    "    # Normalizing distance map\n",
    "    # Can only normalize pre-segmented zone\n",
    "    ret, connected_regions = cv2.connectedComponents(eroded)\n",
    "    normalized_distance_map = distance*0\n",
    "    # Normalize each zone numerated by i\n",
    "    for i in range(1,connected_regions.max()+1):\n",
    "        # Extract zone i\n",
    "        regions_distance = distance*(connected_regions==i)\n",
    "        # Normalize this zone\n",
    "        normalize_factor = 1/regions_distance.max()\n",
    "        # Add normalized zone to the new distance map\n",
    "        normalized_distance_map = normalized_distance_map + regions_distance*normalize_factor\n",
    "    # Thresholdding the Distance map\n",
    "    distance_map_erosed = ski.morphology.reconstruction(normalized_distance_map ,normalized_distance_map - normalized_distance_map.max()*dist_max_threshold\n",
    "                                                    ,method = 'erosion')\n",
    "    # Find Local maximum\n",
    "    coords = peak_local_max(distance_map_erosed, min_distance = min_distance)\n",
    "    # Seeds for Watershed segmentation\n",
    "    mask = np.zeros(normalized_distance_map.shape, dtype=bool)\n",
    "    mask[tuple(coords.T)] = True\n",
    "    markers, _ = ndi.label(mask)\n",
    "    # Watershed function\n",
    "    labels_ws = watershed(-normalized_distance_map, markers, mask=name_img)\n",
    "    # Color label segmented image\n",
    "    img2=color.label2rgb(labels_ws,bg_label=0)\n",
    "    # Plotting results\n",
    "    fig, ax = plt.subplots(1,2,figsize=(12, 12))\n",
    "    ax[0].imshow(name_img,cmap='gray')\n",
    "    ax[0].axis('off') \n",
    "    ax[0].set_title(\"Binarized image\")\n",
    "\n",
    "    ax[1].set_title(\"Segmented image with labeled region\")\n",
    "    ax[1].imshow(img2)\n",
    "    ax[1].axis('off')   \n",
    "    return labels_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140b3d7-bc51-449e-89fd-2815e4fa82b1",
   "metadata": {},
   "source": [
    "### 2. Watershed with Sure Foreground Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94da585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watershed segmentation with fore ground extraction\n",
    "def NP_segmentation_fg_bg(name_img, dist_max_threshold = 0.4,erode_iter=1,open_iter=0,kernel_size = 3) :\n",
    "    # Filter kernel for morphological operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size,kernel_size))\n",
    "    # Eroding morphology - Reduce isotropically object size\n",
    "    # This operation helps a lot in revealing the line between object\n",
    "    # But it can also clear out object with small size\n",
    "    eroded = cv2.erode(name_img, kernel, iterations=erode_iter)\n",
    "    eroded = cv2.morphologyEx(eroded, cv2.MORPH_OPEN, kernel, iterations=open_iter)\n",
    "\n",
    "    # Distance map transform with edt (Euclidean Distance Transform)\n",
    "    distance = ndi.distance_transform_edt(eroded)\n",
    "    # Normalizing distance map\n",
    "    # Can only normalize pre-segmented zone\n",
    "    ret, connected_regions = cv2.connectedComponents(eroded)\n",
    "    normalized_distance_map = distance*0\n",
    "    # Normalize each zone numerated by i\n",
    "    for i in range(1,connected_regions.max()+1):\n",
    "        # Extract zone i\n",
    "        regions_distance = distance*(connected_regions==i)\n",
    "        # Normalize this zone\n",
    "        normalize_factor = 1/regions_distance.max()\n",
    "        # Add normalized zone to the new distance map\n",
    "        normalized_distance_map = normalized_distance_map + regions_distance*normalize_factor\n",
    "\n",
    "    # Sure back ground using dilating morphology\n",
    "    # Dilating morphology not exploited in this algorithm so the sure back ground is simply the processed image\n",
    "    sure_bg = cv2.dilate(name_img,kernel, iterations= 0)\n",
    "    # Sure for ground extraction by thresholdding normalized distance map\n",
    "    ret2,sure_fg = cv2.threshold(normalized_distance_map,dist_max_threshold*normalized_distance_map.max(),255,cv2.THRESH_BINARY)\n",
    "    # Convert float to unsigned integer\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    # Unknown zone = Sure back ground - Sure fore ground\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    # Seeds for Watershed segmentation\n",
    "    mask_fg = np.zeros(sure_fg.shape, dtype=bool)\n",
    "    mask_fg[sure_fg != 0] = True \n",
    "    markers, _ = ndi.label(mask_fg)\n",
    "    # Watershed segmentation algorithm\n",
    "    labels_ws = watershed(-normalized_distance_map, markers,mask=name_img)\n",
    "    # Color labeling  seeds\n",
    "    markers_plot=color.label2rgb(markers,bg_label=0)\n",
    "    # Color labeling segmented image\n",
    "    img2=color.label2rgb(labels_ws,bg_label=0)\\\n",
    "    # Plotting results\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "    gs = fig.add_gridspec(2,2)\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    ax1.imshow(name_img,cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Binarized image\",weight=\"bold\")\n",
    "    \n",
    "    ax2.imshow(markers_plot)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Extracted Sure Foreground (Seeds)\", weight=\"bold\")\n",
    "    \n",
    "    ax3.imshow(img2)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title(\"Segmented image with labeled regions\",weight=\"bold\")\n",
    "\n",
    "    return labels_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce59be6-4f05-489e-8edd-aa31c3021b74",
   "metadata": {},
   "source": [
    "# Size Histogram and Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a6ace-0c4d-4a53-a2b5-5049cff91626",
   "metadata": {},
   "source": [
    "### 1. Size histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3c00fa-3d4f-4983-ae57-5e5384b31c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size histogram plotting \n",
    "def size_histogram(labels_ws, pixel_to_nm, name_img,bins = 100) : \n",
    "    #\n",
    "    # Variables:\n",
    "        # labels_ws : image after segmentation processing (NP_segmentation)\n",
    "        # pixel_to_nm : exchange rate between pixels and scale bar (detect_scale_bar)\n",
    "        # name_img : the original image\n",
    "    # Read image\n",
    "    img = cv2.imread(name_img)\n",
    "    # Measure segmented regions properties\n",
    "    regions = measure.regionprops(labels_ws, intensity_image=img)\n",
    "    # Image to plot result\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    labels_ws = labels_ws > 0\n",
    "    back_ground = gray_image*0 +255\n",
    "    img_label = gray_image*labels_ws\n",
    "    img_label= img_label+back_ground\n",
    "    # Insert sizes into an array\n",
    "    i=0\n",
    "    radius = np.array([])\n",
    "\n",
    "    for regions_prop in regions :\n",
    "        radius = np.append(radius,np.sqrt(regions_prop['Area']*pixel_to_nm**2/np.pi)) # Assume that np is circle\n",
    "    # Plotting results\n",
    "    fig, axs = plt.subplots(1,2,figsize=(21, 7))\n",
    "    \n",
    "    axs[0].imshow(img_label,cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Original Extracted Image\")\n",
    "    \n",
    "    axs[1].hist(radius,bins = bins)\n",
    "    axs[1].set_xlabel(\"Radius(nm)\")\n",
    "    axs[1].set_ylabel(\"Probability\")\n",
    "    axs[1].set_title(\"Size Histogram\")\n",
    "    axs[1].grid('on')\n",
    "            \n",
    "    return radius\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b46cc-881a-4566-b186-2068a004206b",
   "metadata": {},
   "source": [
    "### 2. Hsitogram dividision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11ee77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_histogram(radius, edge_radius) :\n",
    "    #\n",
    "    # Variables:\n",
    "        # radius : The radius of each NP finded by radius_histogram\n",
    "        # edge_radius : This parametre is needed to define different group of NPs, we define it after using radius_histogram\n",
    "    \n",
    "    radius1 = np.array([])\n",
    "    for rad in radius:\n",
    "         if rad < edge_radius:\n",
    "             radius1 = np.append(radius1,rad)\n",
    "\n",
    "    radius2 = np.array([])\n",
    "    for rad in radius:\n",
    "         if rad > edge_radius  :\n",
    "             radius2 = np.append(radius2,rad)\n",
    "    \n",
    "    return (radius1 , radius2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4065a5-bab8-4196-b58b-dfd54b70b9e6",
   "metadata": {},
   "source": [
    "### 3. Gaussian Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a17fc7-aed0-4cbe-b1e9-3b267e614cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian distribution function\n",
    "def gaus(X,C,X_mean,sigma):\n",
    "    return C*np.exp(-(X-X_mean)**2/(2*sigma**2))\n",
    "\n",
    "# Gaussian curve fir algorithm\n",
    "# In this approximation algorithm, we create the histogram with interval calculated from min and max value which can be mathematically incorrect\n",
    "# User can optimize the histogram construction with 68–95–99.7 rule.\n",
    "def Gaussian_fit(radius, bins):\n",
    "    mean = np.mean(radius)\n",
    "    variance = np.var(radius)\n",
    "    # Histogram construction with intervals calculated from min and max value\n",
    "    hist, bin_edges = np.histogram(radius,density='True',bins=bins, range = (0.8*np.min(radius),1.2*np.max(radius)))\n",
    "    # Normalize histogram\n",
    "    hist=hist/sum(hist)\n",
    "    # Change histogram to discreted data points [x = 1/2(left value + right value);y = y histo]\n",
    "    n = len(hist)\n",
    "    x_hist=np.zeros((n),dtype=float) \n",
    "    for ii in range(n):\n",
    "        x_hist[ii]=(bin_edges[ii+1]+bin_edges[ii])/2\n",
    "    y_hist=hist\n",
    "    # Initializing Gaussian fit parameters\n",
    "    mean = sum(x_hist*y_hist)/sum(y_hist)                  \n",
    "    sigma = sum(y_hist*(x_hist-mean)**2)/sum(y_hist) \n",
    "    # Gaussian curve fit\n",
    "    param_optimised,param_covariance_matrix = curve_fit(gaus,x_hist,y_hist,p0=[max(y_hist),mean,sigma],maxfev=5000)\n",
    "    return param_optimised,param_covariance_matrix,x_hist, y_hist\n",
    "# Plotting Gaussian fit on Histogram constructed\n",
    "def plot_Gaussian_fit(radius, bins):\n",
    "    # Gaussian fitting\n",
    "    param_optimised,param_covariance_matrix,x_hist,y_hist = Gaussian_fit(radius,bins)\n",
    "    # Plotting result\n",
    "    plt.figure()\n",
    "    x_hist_2=np.linspace(np.min(x_hist),np.max(x_hist),500)\n",
    "    plt.plot(x_hist_2,gaus(x_hist_2,*param_optimised),'r',label='Gaussian fit')\n",
    "    plt.legend()\n",
    "    # Calculted size\n",
    "    str1 = \"<r> =\" + str(param_optimised[1]) + \"+-\"+str(np.sqrt(param_covariance_matrix[1,1]))+' nm '\n",
    "    print(str1)\n",
    "\n",
    "    weights = np.ones_like(radius) / len(radius)\n",
    "    plt.hist(radius, weights=weights,bins = bins)\n",
    "    plt.title(str1,weight = \"bold\")\n",
    "    plt.xlabel(\"Radius(nm)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.grid('on')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c9504-7a7f-4028-bb43-d80b9136d708",
   "metadata": {},
   "source": [
    "# Nanoparticles extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3c7890-f1a1-4761-8afa-9264e6bab59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract nanoparticles form original image\n",
    "def extract_np(i,img, labels_ws,black_bg_color = False):\n",
    "     # Variables:\n",
    "        # i : Integer numerating the nanoparticle\n",
    "        # img : Original image\n",
    "        # labels_ws : Segmented mask\n",
    "        # black_bg_color : indicate the color of the background (correctly correspond to model) \n",
    "    # Preparing extracted image\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    back_ground = gray_image*0 + 255\n",
    "    back_ground_gray_image = back_ground*(labels_ws!=i)\n",
    "    \n",
    "    # Extract the i-th np mask\n",
    "    extracted = labels_ws*(labels_ws==i)\n",
    "    # Extract the i-th np from original image\n",
    "    gray_image_1np = gray_image*(labels_ws==i) \n",
    "    gray_image_1np = gray_image_1np + back_ground_gray_image\n",
    "\n",
    "    # Change type to unsigned int form float\n",
    "    extracted =np.uint8(extracted)\n",
    "    # Crop out the nanoparticle\n",
    "    # Find the contour of np\n",
    "    cnts = cv2.findContours(extracted, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    # Locating left, right, top, bottom extremum of the np\n",
    "    left_extreme = tuple(c[c[:, :, 0].argmin()][0])     \n",
    "    right_extreme = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    top_extreme = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    bottom_extreme = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    # Crop out the np from located extremum\n",
    "    extracted_np = gray_image_1np[top_extreme[1]:bottom_extreme[1],left_extreme[0]:right_extreme[0]]\n",
    "    # Padding np image for the better classification (size image = 120%)\n",
    "    h_old = extracted_np.shape[0]\n",
    "    w_old = extracted_np.shape[1]\n",
    "    h = math.floor(extracted_np.shape[0]*1.2)\n",
    "    w = math.floor(extracted_np.shape[1]*1.2)\n",
    "    calibrated_image  = np.full((h,w),255, dtype=np.uint8)\n",
    "    x_center = (w - w_old) // 2\n",
    "    y_center = (h - h_old) // 2\n",
    "    calibrated_image[y_center:y_center+h_old, \n",
    "                     x_center:x_center+w_old] = extracted_np\n",
    "    # Change back ground color if needed\n",
    "    if black_bg_color:\n",
    "        calibrated_image = 255 - calibrated_image\n",
    "    # Convert array to image\n",
    "    calibrated_image = Image.fromarray(calibrated_image)\n",
    "    return calibrated_image    \n",
    "\n",
    "# Extract binarized mask\n",
    "# Same principle as the previous extraction\n",
    "def extract_binary_np(i,img, labels_ws,black_bg_color = False):\n",
    "    \n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    back_ground = gray_image*0 +255\n",
    "    back_ground_gray_image = back_ground*(labels_ws!=i)\n",
    "    \n",
    "    extracted = labels_ws*(labels_ws==i)\n",
    "    \n",
    "    binary_image_1np = extracted\n",
    "    binary_image_1np = binary_image_1np + back_ground_gray_image\n",
    "    \n",
    "    extracted =np.uint8(extracted)\n",
    "    \n",
    "    cnts = cv2.findContours(extracted, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "    \n",
    "    left_extreme = tuple(c[c[:, :, 0].argmin()][0])     \n",
    "    right_extreme = tuple(c[c[:, :, 0].argmax()][0])\n",
    "    top_extreme = tuple(c[c[:, :, 1].argmin()][0])\n",
    "    bottom_extreme = tuple(c[c[:, :, 1].argmax()][0])\n",
    "    \n",
    "    extracted_np = binary_image_1np[top_extreme[1]:bottom_extreme[1],left_extreme[0]:right_extreme[0]]\n",
    "    \n",
    "    h_old = extracted_np.shape[0]\n",
    "    w_old = extracted_np.shape[1]\n",
    "    \n",
    "    h = math.floor(extracted_np.shape[0]*1.2)\n",
    "    w = math.floor(extracted_np.shape[1]*1.2)\n",
    "    \n",
    "    calibrated_image  = np.full((h,w),255, dtype=np.uint8)\n",
    "    \n",
    "    x_center = (w - w_old) // 2\n",
    "    y_center = (h - h_old) // 2\n",
    "    \n",
    "    calibrated_image[y_center:y_center+h_old, \n",
    "                     x_center:x_center+w_old] =  extracted_np\n",
    "    if black_bg_color:\n",
    "        calibrated_image = 255 - calibrated_image\n",
    "\n",
    "    calibrated_image = Image.fromarray(calibrated_image)\n",
    "    \n",
    "    return calibrated_image    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cf6ae-bf0b-4b81-a665-a59c2aac23f1",
   "metadata": {},
   "source": [
    "# Classification function and Explaination AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33130fd7-36d6-4597-ae20-751e2f0ef86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify an image with a model\n",
    "def testing_image(img,model,target_size=(256,256),color_mode = 'L'):\n",
    "    # Variables:\n",
    "        # img : Input image\n",
    "        # model : Model used to classify image\n",
    "        # target_size : Target size of the original image, need to be matched with model's input\n",
    "        # color_mode : 'L' for grayscale and 'RGB' for RGB, need to be matched with model's input\n",
    "    # loading testing image with the target size for the image\n",
    "    test_image = img.resize(target_size)\n",
    "    # loading testing image with the target size for the image\n",
    "    test_image = test_image.convert(mode=color_mode)\n",
    "    # converts image into an array\n",
    "    test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
    "    # expands array (from converted image) with a new dimension (for classifying values)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    test_image = tf.keras.applications.inception_v3.preprocess_input(test_image)\n",
    "    # making prediction based on test_image and labeling it results\n",
    "    result = model.predict(x = test_image)\n",
    "    # printing predictions\n",
    "    print(result)\n",
    "\n",
    "    return result, test_image\n",
    "    \n",
    "# AI explainable via Xplique\n",
    "# This Xplique function is only compatible for this whole process and notebook \"Example of usage\"\n",
    "# In case of using Xplique for a specific image, please check out the examples of Xplique notebook\n",
    "def show_xplique(model,img,label,total_label,alpha,method):\n",
    "     # Variables:\n",
    "        # img : Input image\n",
    "        # model : Model used to classify image\n",
    "        # label : explained label, usually equal np.argmax(result)\n",
    "        # total_label : total number of classes\n",
    "        # alpha : superimposed intensity of explained map onto original image\n",
    "        # method : explainer method (GradientInput, Saliency,...)\n",
    "    X = np.squeeze(img, axis = 0)\n",
    "    explainer = method(model)\n",
    "    Y = []\n",
    "    labels = tf.keras.utils.to_categorical(label,total_label)\n",
    "    Y.append(labels)\n",
    "    explanations = explainer(img, Y)\n",
    "    to_show = np.squeeze(explanations,axis=0)\n",
    "    # Plotting result\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.imshow(X)\n",
    "    plt.imshow(to_show, cmap=\"jet\", alpha=alpha)\n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22372db6-bca8-4ad8-8765-7ac36a4cac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification function\n",
    "# This function classifies then returns a list of extracted labels correspond to different classes\n",
    "def Classification(img_name,model,total_label,labels_ws,target_size=(256,256),color_mode='L',black_bg_color = False):\n",
    "     # Variables:\n",
    "        # img_name : Input image\n",
    "        # model : Model used to classify image\n",
    "        # total_label : total number of classes\n",
    "        # labels_ws : Watershed label\n",
    "        # target_size : Target size of the original image, need to be matched with model's input\n",
    "        # color_mode : 'L' for grayscale and 'RGB' for RGB, need to be matched with model's input\n",
    "        # black_bg_color : indicate the color of the background (correctly correspond to model) \n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Intialize list for labels and classification result of each nanoparticle\n",
    "    labels = []\n",
    "    results = []\n",
    "    # Classify all objects and put the results in a list\n",
    "    for i in range(1,labels_ws.max()+1):\n",
    "        np_i = extract_np(i,img, labels_ws,black_bg_color = black_bg_color)\n",
    "        result, test_image = testing_image(np_i,model,target_size,color_mode)\n",
    "        results.append(np.argmax(result))\n",
    "    # Testing for each class\n",
    "    for r in range(0,total_label):\n",
    "        extracted = gray_image*0 +255\n",
    "        labels_extracted = labels_ws*0\n",
    "        index = 1\n",
    "    # Testing for each object if their labeled class corresponds to the tested one\n",
    "        for j in results:\n",
    "            # If yes => add the object to a new label\n",
    "            if j == r:\n",
    "                extracted += gray_image*(labels_ws==index)\n",
    "                labels_extracted += labels_ws*(labels_ws==index)\n",
    "            # index indicate the number labeled of the selected object\n",
    "            index += 1\n",
    "        # Add the extracted label numerated by j to the label list after testing\n",
    "        labels.append(labels_extracted)\n",
    "        # Plotting the classified labels\n",
    "        plt.figure()\n",
    "        plt.title('Labeled class ' + str(r))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(extracted,cmap='gray')\n",
    "        plt.show()\n",
    "    return labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
